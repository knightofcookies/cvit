{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "95039d9c-1199-48bf-8d08-44c5cad555e3",
    "_uuid": "d65710f7-742e-45f8-bed6-98c12057d0c4",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Install packages\n",
    "*inspired from Unsloth's Deepseek-OCR fine-tuning notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "9139ed7f-6790-490a-9a62-13d0d6d3af9f",
    "_uuid": "2a87e304-cfd2-4f93-b7c2-d2e18ef1dd55",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-20T07:08:31.795120Z",
     "iopub.status.busy": "2026-01-20T07:08:31.794934Z",
     "iopub.status.idle": "2026-01-20T07:09:41.580076Z",
     "shell.execute_reply": "2026-01-20T07:09:41.579229Z",
     "shell.execute_reply.started": "2026-01-20T07:08:31.795100Z"
    },
    "id": "be6b78f2",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Collecting xformers==0.0.32.post2\n",
      "  Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Collecting trl\n",
      "  Downloading trl-0.27.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
      "Collecting cut_cross_entropy\n",
      "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2026.1.3-py3-none-any.whl.metadata (32 kB)\n",
      "Downloading xformers-0.0.32.post2-cp39-abi3-manylinux_2_28_x86_64.whl (117.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.2/117.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trl-0.27.0-py3-none-any.whl (532 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m532.5/532.5 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
      "Downloading unsloth_zoo-2026.1.3-py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xformers, unsloth_zoo, trl, cut_cross_entropy, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.49.1 cut_cross_entropy-25.1.1 trl-0.27.0 unsloth_zoo-2026.1.3 xformers-0.0.32.post2\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (5.29.5)\n",
      "Collecting datasets==4.3.0\n",
      "  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (0.36.0)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (3.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (3.6.0)\n",
      "Collecting multiprocess<0.70.17 (from datasets==4.3.0)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (26.0rc2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==4.3.0) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0) (1.2.1rc0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets==4.3.0) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets==4.3.0) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.3.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==4.3.0) (2.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.3.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.3.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==4.3.0) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.3.0) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==4.3.0) (1.17.0)\n",
      "Downloading datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: multiprocess, fsspec, datasets\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.18\n",
      "    Uninstalling multiprocess-0.70.18:\n",
      "      Successfully uninstalled multiprocess-0.70.18\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.10.0\n",
      "    Uninstalling fsspec-2025.10.0:\n",
      "      Successfully uninstalled fsspec-2025.10.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.4.2\n",
      "    Uninstalling datasets-4.4.2:\n",
      "      Successfully uninstalled datasets-4.4.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth-zoo 2026.1.3 requires msgspec, which is not installed.\n",
      "unsloth-zoo 2026.1.3 requires tyro, which is not installed.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "unsloth-zoo 2026.1.3 requires peft!=0.11.0,>=0.18.0, but you have peft 0.17.1 which is incompatible.\n",
      "unsloth-zoo 2026.1.3 requires torchao>=0.13.0, but you have torchao 0.10.0 which is incompatible.\n",
      "unsloth-zoo 2026.1.3 requires trl!=0.19.0,<=0.24.0,>=0.18.2, but you have trl 0.27.0 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.3.0 fsspec-2025.9.0 multiprocess-0.70.16\n",
      "Collecting unsloth\n",
      "  Downloading unsloth-2026.1.3-py3-none-any.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unsloth-2026.1.3-py3-none-any.whl (389 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.2/389.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unsloth\n",
      "Successfully installed unsloth-2026.1.3\n",
      "Collecting transformers==4.56.2\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (26.0rc2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.1rc0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2026.1.4)\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.1\n",
      "    Uninstalling transformers-4.57.1:\n",
      "      Successfully uninstalled transformers-4.57.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unsloth 2026.1.3 requires tyro, which is not installed.\n",
      "unsloth-zoo 2026.1.3 requires msgspec, which is not installed.\n",
      "unsloth-zoo 2026.1.3 requires tyro, which is not installed.\n",
      "unsloth 2026.1.3 requires peft!=0.11.0,>=0.18.0, but you have peft 0.17.1 which is incompatible.\n",
      "unsloth 2026.1.3 requires trl!=0.19.0,<=0.24.0,>=0.18.2, but you have trl 0.27.0 which is incompatible.\n",
      "unsloth-zoo 2026.1.3 requires peft!=0.11.0,>=0.18.0, but you have peft 0.17.1 which is incompatible.\n",
      "unsloth-zoo 2026.1.3 requires torchao>=0.13.0, but you have torchao 0.10.0 which is incompatible.\n",
      "unsloth-zoo 2026.1.3 requires trl!=0.19.0,<=0.24.0,>=0.18.2, but you have trl 0.27.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed transformers-4.56.2\n",
      "Collecting trl==0.22.2\n",
      "  Downloading trl-0.22.2-py3-none-any.whl.metadata (11 kB)\n",
      "Downloading trl-0.22.2-py3-none-any.whl (544 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: trl\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.27.0\n",
      "    Uninstalling trl-0.27.0:\n",
      "      Successfully uninstalled trl-0.27.0\n",
      "Successfully installed trl-0.22.2\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
      "Collecting addict\n",
      "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (1.13)\n",
      "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Installing collected packages: addict\n",
      "Successfully installed addict-2.4.0\n",
      "Collecting verovio\n",
      "  Downloading verovio-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.2 kB)\n",
      "Downloading verovio-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: verovio\n",
      "Successfully installed verovio-5.7.0\n",
      "Collecting faker\n",
      "  Downloading faker-40.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Downloading faker-40.1.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faker\n",
      "Successfully installed faker-40.1.2\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (26.0rc2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.56.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.9.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.1rc0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2026.1.4)\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2\n",
    "!pip install jiwer\n",
    "!pip install einops addict easydict\n",
    "!pip install verovio\n",
    "!pip install faker\n",
    "!pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "64f42f58-1a0a-48b4-b06a-64eae1dc2cf7",
    "_uuid": "048cb460-411a-4c3f-9692-f600d4ed17e8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Download Deepseek-OCR from Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "e03f0851-f972-4774-9ebc-f0533235909d",
    "_uuid": "b50c937a-de97-4648-992f-7cc493db40b8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-20T07:09:41.582900Z",
     "iopub.status.busy": "2026-01-20T07:09:41.582326Z",
     "iopub.status.idle": "2026-01-20T07:10:25.116518Z",
     "shell.execute_reply": "2026-01-20T07:10:25.115597Z",
     "shell.execute_reply.started": "2026-01-20T07:09:41.582868Z"
    },
    "id": "9d7036ae",
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb5a52c2bbf4eb6b620e596a3cd2e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5658c777321648fbaaad7b4d95196959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "assets/fig1.png:   0%|          | 0.00/396k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea2c2c0015e4398b37a4726bd55083a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "assets/show3.jpg:   0%|          | 0.00/247k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3238e0eb17654ed5904818de6a288bfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "assets/show1.jpg:   0%|          | 0.00/117k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fa458e3d8e4552b1bbdb251e75baaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "assets/show2.jpg:   0%|          | 0.00/216k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db330bb7e3a241ea8221358b50d947c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff10f5bd93854678ae6c01140bcfa365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b79ec10625554c529938bf3ee2c15e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390d5ae7563b44b88a046164ae93c441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README-checkpoint.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03f613ab81d4651b260936b141545ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_deepseek_v2.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c6297924194df99d531101da104f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "assets/show4.jpg:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9092600f194fc1aa0e2084085267ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4cf9179d1fc4b74a1fec148d4b4fa36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "conversation.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbadf98a62d415e834276a9f3989326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "deepencoder.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbabb249303472bb30d590bab313d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3778bfa88b94f118c83f1bc01489fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_deepseekocr.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa55d65051e4a37a81c3af1d917db47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5511a100539e48fda1882eeb698f2401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e657caf4444fbbb4d947d841c30352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_deepseekv2.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b509d65047540a1a977607a43328ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa607599ec964621b51522fb777ffa77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f3593d30af42e18f2f13131c345112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/deepseek_ocr'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Login for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T07:10:25.118005Z",
     "iopub.status.busy": "2026-01-20T07:10:25.117642Z",
     "iopub.status.idle": "2026-01-20T07:10:25.273698Z",
     "shell.execute_reply": "2026-01-20T07:10:25.273103Z",
     "shell.execute_reply.started": "2026-01-20T07:10:25.117979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotfix (Transformers >= 4.46 compatibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T07:10:25.274786Z",
     "iopub.status.busy": "2026-01-20T07:10:25.274511Z",
     "iopub.status.idle": "2026-01-20T07:11:12.781049Z",
     "shell.execute_reply": "2026-01-20T07:11:12.780370Z",
     "shell.execute_reply.started": "2026-01-20T07:10:25.274751Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-20 07:10:52.349191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768893052.778274      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768893052.913393      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768893053.982775      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768893053.982811      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768893053.982814      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768893053.982817      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Monkeypatching LlamaFlashAttention2 for DeepSeek-OCR compatibility...\n"
     ]
    }
   ],
   "source": [
    "import transformers.models.llama.modeling_llama\n",
    "if not hasattr(transformers.models.llama.modeling_llama, \"LlamaFlashAttention2\"):\n",
    "    print(\">>> Monkeypatching LlamaFlashAttention2 for DeepSeek-OCR compatibility...\")\n",
    "    transformers.models.llama.modeling_llama.LlamaFlashAttention2 = transformers.models.llama.modeling_llama.LlamaAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "56cef893-9348-4674-9344-09a9cfce1913",
    "_uuid": "bae776ed-97ae-489a-b32e-67c404000518",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-20T07:18:25.202021Z",
     "iopub.status.busy": "2026-01-20T07:18:25.201448Z",
     "iopub.status.idle": "2026-01-20T07:18:25.207384Z",
     "shell.execute_reply": "2026-01-20T07:18:25.206735Z",
     "shell.execute_reply.started": "2026-01-20T07:18:25.201990Z"
    },
    "id": "4399efb5",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "c2a9e462-a590-4f01-b30b-c40e53ee20cc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import torchvision.transforms as T\n",
    "import warnings\n",
    "import gc\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from faker import Faker\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import types\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projector MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T07:29:25.206535Z",
     "iopub.status.busy": "2026-01-20T07:29:25.205845Z",
     "iopub.status.idle": "2026-01-20T07:29:25.211542Z",
     "shell.execute_reply": "2026-01-20T07:29:25.210827Z",
     "shell.execute_reply.started": "2026-01-20T07:29:25.206492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepSeekOCRToGOTProjector(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim=1024):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(encoder_dim, decoder_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(decoder_dim * 2, decoder_dim),\n",
    "            nn.LayerNorm(decoder_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T07:29:25.213257Z",
     "iopub.status.busy": "2026-01-20T07:29:25.212972Z",
     "iopub.status.idle": "2026-01-20T07:29:25.243749Z",
     "shell.execute_reply": "2026-01-20T07:29:25.243179Z",
     "shell.execute_reply.started": "2026-01-20T07:29:25.213238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DeepSeekGOTFusion(nn.Module):\n",
    "    def __init__(self, deepseek_ocr_path, got_path, tokenizer, use_lora=True):\n",
    "        super().__init__()\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        frozen_dtype = torch.float16\n",
    "        self.vision_dtype = frozen_dtype\n",
    "\n",
    "        print(f\">>> Loading DeepSeek-OCR Encoder...\")\n",
    "        tmp_ds = AutoModel.from_pretrained(\n",
    "            deepseek_ocr_path,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"cpu\",\n",
    "            torch_dtype=frozen_dtype,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "\n",
    "        if hasattr(tmp_ds, \"model\"):\n",
    "            base_model = tmp_ds.model\n",
    "        else:\n",
    "            base_model = tmp_ds\n",
    "\n",
    "        if hasattr(base_model, \"deep_encoder\"):\n",
    "            self.vision_tower = base_model.deep_encoder\n",
    "        elif hasattr(base_model, \"vision_model\"):\n",
    "            self.vision_tower = base_model.vision_model\n",
    "        else:\n",
    "            self.vision_tower = base_model\n",
    "\n",
    "        self.vision_dim = 1024\n",
    "        if hasattr(self.vision_tower, \"config\"):\n",
    "            self.vision_dim = getattr(self.vision_tower.config, \"hidden_size\", 1024)\n",
    "        \n",
    "        # Correct for DeepSeek-OCR specific logic if it erroneously picks up LLM dim - bugfix\n",
    "        if self.vision_dim == 1280:\n",
    "            print(\">>> Correcting detected dimension 1280 -> 1024 for DeepEncoder compatibility.\")\n",
    "            self.vision_dim = 1024\n",
    "\n",
    "        print(f\">>> Vision Dimension: {self.vision_dim}\")\n",
    "        \n",
    "        self.vision_tower = self.vision_tower.to(\"cuda\")\n",
    "        del tmp_ds\n",
    "        gc.collect()\n",
    "\n",
    "        print(f\">>> Loading GOT-OCR Decoder...\")\n",
    "        tmp_got = AutoModel.from_pretrained(\n",
    "            got_path,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"cpu\",\n",
    "            torch_dtype=frozen_dtype,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "\n",
    "        if hasattr(tmp_got, \"language_model\"):\n",
    "            self.decoder = tmp_got.language_model\n",
    "        else:\n",
    "            self.decoder = tmp_got.model\n",
    "\n",
    "        self.decoder_dim = self.decoder.config.hidden_size\n",
    "        self.decoder = self.decoder.to(\"cuda\")\n",
    "\n",
    "        self.lm_head = None\n",
    "        if hasattr(tmp_got, \"lm_head\"):\n",
    "            self.lm_head = tmp_got.lm_head\n",
    "        elif hasattr(self.decoder, \"lm_head\"):\n",
    "            self.lm_head = self.decoder.lm_head\n",
    "        \n",
    "        if self.lm_head is None:\n",
    "            vocab_size = self.decoder.config.vocab_size\n",
    "            self.lm_head = nn.Linear(self.decoder_dim, vocab_size, bias=False)\n",
    "            if hasattr(self.decoder, \"embed_tokens\"):\n",
    "                self.lm_head.weight = self.decoder.embed_tokens.weight\n",
    "            elif hasattr(self.decoder, \"wte\"):\n",
    "                self.lm_head.weight = self.decoder.wte.weight\n",
    "        \n",
    "        self.lm_head = self.lm_head.to(dtype=frozen_dtype, device=\"cuda\")\n",
    "        del tmp_got\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # patch - bugfix\n",
    "        if not hasattr(self.decoder, \"prepare_inputs_for_generation\"):\n",
    "            def _prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None,\n",
    "                                               token_type_ids=None, use_cache=None, **kwargs):\n",
    "                return {\n",
    "                    \"input_ids\": input_ids,\n",
    "                    \"past_key_values\": past_key_values,\n",
    "                    \"attention_mask\": attention_mask,\n",
    "                    \"token_type_ids\": token_type_ids,\n",
    "                    \"use_cache\": use_cache,\n",
    "                    **kwargs\n",
    "                }\n",
    "            self.decoder.prepare_inputs_for_generation = types.MethodType(_prepare_inputs_for_generation, self.decoder)\n",
    "        \n",
    "        if use_lora:\n",
    "            print(\">>> Injecting LoRA Adapters...\")\n",
    "            lora_config = LoraConfig(\n",
    "                r=16, \n",
    "                lora_alpha=32, \n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n",
    "                lora_dropout=0.05, \n",
    "                bias=\"none\", \n",
    "                task_type=\"CAUSAL_LM\"\n",
    "            )\n",
    "            self.decoder = get_peft_model(self.decoder, lora_config)\n",
    "            self.decoder.print_trainable_parameters()\n",
    "\n",
    "        self.projector = DeepSeekOCRToGOTProjector(self.vision_dim, self.decoder_dim)\n",
    "        \n",
    "        self.projector.apply(self._init_weights)\n",
    "        \n",
    "        self.img_start_id = tokenizer.convert_tokens_to_ids(\"<img>\") # verified\n",
    "        self.img_end_id = tokenizer.convert_tokens_to_ids(\"</img>\") # verified\n",
    "\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "        self.lm_head.requires_grad_(False)\n",
    "        self.projector.requires_grad_(True)\n",
    "        self.decoder.requires_grad_(False)\n",
    "        self.projector = self.projector.to(device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.01)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
    "        if hasattr(self.decoder, \"gradient_checkpointing_enable\"):\n",
    "            self.decoder.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        if not os.path.exists(save_directory): os.makedirs(save_directory)\n",
    "        torch.save(self.projector.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n",
    "        with open(os.path.join(save_directory, \"config.json\"), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"vision_dim\": self.vision_dim, \n",
    "                \"decoder_dim\": self.decoder_dim,\n",
    "                \"architecture\": \"DeepSeekOCRToGOTProjector\"\n",
    "            }, f, indent=4)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None, **kwargs):\n",
    "        # apparent bug fix but I'm not so sure\n",
    "        # kwargs.pop(\"labels\", None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                vision_out = self.vision_tower(pixel_values.to(self.vision_dtype), patch_embeds=None)\n",
    "            except TypeError:\n",
    "                vision_out = self.vision_tower(pixel_values.to(self.vision_dtype))\n",
    "            \n",
    "            if isinstance(vision_out, (tuple, list)):\n",
    "                features = vision_out[0]\n",
    "            elif hasattr(vision_out, \"last_hidden_state\"):\n",
    "                features = vision_out.last_hidden_state\n",
    "            else:\n",
    "                features = vision_out\n",
    "            features = features.detach()\n",
    "\n",
    "        vision_embeds = self.projector(features.to(torch.float32)).to(self.vision_dtype)\n",
    "        B, N, _ = vision_embeds.shape\n",
    "        device = vision_embeds.device\n",
    "\n",
    "        input_emb_fn = self.decoder.get_input_embeddings()\n",
    "        start_embeds = input_emb_fn(torch.tensor([self.img_start_id], device=device)).expand(B, 1, -1)\n",
    "        end_embeds = input_emb_fn(torch.tensor([self.img_end_id], device=device)).expand(B, 1, -1)\n",
    "        text_embeds = input_emb_fn(input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat([start_embeds, vision_embeds, end_embeds, text_embeds], dim=1)\n",
    "        \n",
    "        vision_len = N + 2\n",
    "        full_mask = None\n",
    "        if attention_mask is not None:\n",
    "            v_mask = torch.ones((B, vision_len), device=device, dtype=attention_mask.dtype)\n",
    "            full_mask = torch.cat([v_mask, attention_mask], dim=1)\n",
    "\n",
    "        outputs = self.decoder(\n",
    "            input_ids=torch.zeros((B, inputs_embeds.shape[1]), device=device, dtype=torch.long),\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=full_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        relevant_hidden = hidden_states[:, vision_len-1:-1, :].contiguous()\n",
    "        if self.lm_head is not None:\n",
    "            logits = self.lm_head(relevant_hidden)\n",
    "        else:\n",
    "            raise ValueError(\"LM Head is not defined.\")\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.decoder.config.vocab_size).to(torch.float32), labels.view(-1))\n",
    "\n",
    "        return CausalLMOutputWithPast(loss=loss, logits=logits if not self.training else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T07:29:25.357034Z",
     "iopub.status.busy": "2026-01-20T07:29:25.356471Z",
     "iopub.status.idle": "2026-01-20T07:29:25.367871Z",
     "shell.execute_reply": "2026-01-20T07:29:25.367212Z",
     "shell.execute_reply.started": "2026-01-20T07:29:25.357013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RealTextOCRDataset(Dataset):\n",
    "    def __init__(self, tokenizer, num_samples=20000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.fake = Faker('en_US')  # English generator\n",
    "        \n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((1024, 1024), interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                        std=[0.26862954, 0.26130258, 0.27577711])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def generate_content(self):\n",
    "        # Generate varied content types to make the model robust\n",
    "        r = random.random()\n",
    "        if r < 0.4:\n",
    "            # Type 1: Standard Sentences (The easiest for LLMs)\n",
    "            return self.fake.sentence(nb_words=10)\n",
    "        elif r < 0.7:\n",
    "            # Type 2: Addresses (Structured data)\n",
    "            return self.fake.address().replace('\\n', ', ')\n",
    "        else:\n",
    "            # Type 3: Names and Phone numbers\n",
    "            return f\"{self.fake.name()} - {self.fake.phone_number()}\"\n",
    "\n",
    "    def generate_image(self, text):\n",
    "        # 1. Random Background (White-ish)\n",
    "        bg_color = random.randint(230, 255)\n",
    "        img = Image.new('RGB', (1024, 1024), color=(bg_color, bg_color, bg_color))\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # 2. Font Management\n",
    "        try:\n",
    "            # Attempt to use a larger, clearer font size\n",
    "            font_size = random.randint(40, 80) \n",
    "            font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\", font_size)\n",
    "        except IOError:\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        # 3. Draw Text (Centered-ish)\n",
    "        x = random.randint(50, 100)\n",
    "        y = random.randint(200, 500)\n",
    "        \n",
    "        # Simple text wrapping logic\n",
    "        words = text.split()\n",
    "        current_line = \"\"\n",
    "        for word in words:\n",
    "            if (len(current_line) + len(word)) * (font_size * 0.5) > 800:\n",
    "                draw.text((x, y), current_line, fill=(0, 0, 0), font=font)\n",
    "                y += font_size + 10\n",
    "                current_line = word + \" \"\n",
    "            else:\n",
    "                current_line += word + \" \"\n",
    "        \n",
    "        # Draw the last line\n",
    "        draw.text((x, y), current_line, fill=(0, 0, 0), font=font)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.generate_content()\n",
    "        image = self.generate_image(text)\n",
    "\n",
    "        pixel_values = self.transform(image)\n",
    "        prompt = f\"OCR: {text}{self.tokenizer.eos_token}\"\n",
    "        \n",
    "        # Consistent prefix masking logic from Stage 1 setup\n",
    "        prefix_enc = self.tokenizer(\"OCR: \", add_special_tokens=False)\n",
    "        prefix_len = len(prefix_enc.input_ids)\n",
    "\n",
    "        encodings = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_ids = encodings.input_ids.squeeze(0)\n",
    "        attention_mask = encodings.attention_mask.squeeze(0)\n",
    "        labels = input_ids.clone()\n",
    "        \n",
    "        # Mask prefix and padding\n",
    "        starts_with_bos = (input_ids[0] == self.tokenizer.bos_token_id)\n",
    "        offset = 1 if starts_with_bos else 0\n",
    "        labels[:prefix_len + offset] = -100\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "e6f707a0-31d9-4a86-b394-35970a825bac",
    "_uuid": "f66c216f-45c3-4b13-9cfb-f298fc8c5a90",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-20T07:29:25.369543Z",
     "iopub.status.busy": "2026-01-20T07:29:25.369295Z",
     "iopub.status.idle": "2026-01-20T07:29:25.391580Z",
     "shell.execute_reply": "2026-01-20T07:29:25.390870Z",
     "shell.execute_reply.started": "2026-01-20T07:29:25.369525Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEEPSEEK_OCR_PATH = \"deepseek-ai/DeepSeek-OCR\"\n",
    "GOT_PATH = \"stepfun-ai/GOT-OCR2_0\"\n",
    "OUTPUT_DIR = \"./deepseek_ocr_got_final\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-20T07:29:25.392563Z",
     "iopub.status.busy": "2026-01-20T07:29:25.392347Z",
     "iopub.status.idle": "2026-01-20T07:29:35.474194Z",
     "shell.execute_reply": "2026-01-20T07:29:35.473392Z",
     "shell.execute_reply.started": "2026-01-20T07:29:25.392534Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 1. Loading Tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 2. Initializing RealTextOCRDataset...\n",
      ">>> 3. Initializing Fusion Model...\n",
      ">>> Loading DeepSeek-OCR Encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Vision Dimension: 1024\n",
      ">>> Loading GOT-OCR Decoder...\n",
      ">>> Injecting LoRA Adapters...\n",
      "trainable params: 7,569,408 || all params: 568,098,048 || trainable%: 1.3324\n"
     ]
    }
   ],
   "source": [
    "print(\">>> 1. Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(GOT_PATH, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\">>> 2. Initializing RealTextOCRDataset...\")\n",
    "train_dataset = RealTextOCRDataset(tokenizer, num_samples=20000)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack([x['pixel_values'] for x in batch]),\n",
    "        \"input_ids\": torch.stack([x['input_ids'] for x in batch]),\n",
    "        \"labels\": torch.stack([x['labels'] for x in batch]),\n",
    "        \"attention_mask\": torch.stack([x['attention_mask'] for x in batch])\n",
    "    }\n",
    "    \n",
    "print(\">>> 3. Initializing Fusion Model...\")\n",
    "model = DeepSeekGOTFusion(\n",
    "    DEEPSEEK_OCR_PATH, \n",
    "    GOT_PATH, \n",
    "    tokenizer, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "893ec39e-3a64-4d8c-82d6-c337209884a6",
    "_uuid": "3d95b56c-9a31-4f89-9801-47eb9d3e244f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-20T07:29:35.477227Z",
     "iopub.status.busy": "2026-01-20T07:29:35.476944Z",
     "iopub.status.idle": "2026-01-20T07:29:35.535889Z",
     "shell.execute_reply": "2026-01-20T07:29:35.535359Z",
     "shell.execute_reply.started": "2026-01-20T07:29:35.477201Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_STEPS = 63\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=32,\n",
    "    learning_rate=1e-5,\n",
    "    max_steps=MAX_STEPS,   # Use max_steps for streaming\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,      \n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    prediction_loss_only=True,\n",
    "    max_grad_norm=0.5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=5,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "375283a9-184c-47db-8879-7e3cc13cbd2d",
    "_uuid": "b6ae0192-9dab-495b-8277-b240ebfc4f8f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2026-01-20T07:29:35.537720Z",
     "iopub.status.busy": "2026-01-20T07:29:35.537497Z",
     "iopub.status.idle": "2026-01-20T07:29:37.841583Z",
     "shell.execute_reply": "2026-01-20T07:29:37.840601Z",
     "shell.execute_reply.started": "2026-01-20T07:29:35.537701Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\">>> 4. Starting Training...\")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\">>> Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(f\">>> 5. Saving Projector to {OUTPUT_DIR}...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\">>> Done.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 9289627,
     "sourceId": 14544702,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
