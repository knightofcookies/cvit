{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14544702,"sourceType":"datasetVersion","datasetId":9289627}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install packages\n*inspired from Unsloth's Deepseek-OCR fine-tuning notebook*","metadata":{"_uuid":"03e66fe3-013a-438a-86b7-bab70dc92cd8","_cell_guid":"eeb8feb6-ebe3-43f2-9b17-8d7f73721ee4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\n!pip install jiwer\n!pip install einops addict easydict\n!pip install verovio\n!pip install faker\n!pip install peft","metadata":{"_uuid":"19a34c33-4b3d-48c2-ab13-a22b5967822e","_cell_guid":"20853027-008f-4459-a876-0bb4335d522f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:47:24.692061Z","iopub.execute_input":"2026-01-20T11:47:24.692841Z","iopub.status.idle":"2026-01-20T11:48:00.631744Z","shell.execute_reply.started":"2026-01-20T11:47:24.692811Z","shell.execute_reply":"2026-01-20T11:48:00.630933Z"},"id":"be6b78f2","jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### HF Login for Kaggle","metadata":{"_uuid":"f5c8230b-0307-4bf9-9197-f871f2d1409a","_cell_guid":"c4168e28-f6c3-4bd6-a948-5e373a573aae","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=token)","metadata":{"_uuid":"15726006-b50f-4403-bb62-04ace17bf55d","_cell_guid":"dbc09d59-4911-4354-9a52-4e4030ecfb3f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:48:00.633571Z","iopub.execute_input":"2026-01-20T11:48:00.633809Z","iopub.status.idle":"2026-01-20T11:48:01.343105Z","shell.execute_reply.started":"2026-01-20T11:48:00.633785Z","shell.execute_reply":"2026-01-20T11:48:01.342495Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Hotfix (Transformers >= 4.46 compatibility)","metadata":{"_uuid":"c2733041-b7c9-4d47-84fc-585bc529bf39","_cell_guid":"4c2bbbdc-a599-4102-974b-3f0047fe4f15","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import transformers.models.llama.modeling_llama\nif not hasattr(transformers.models.llama.modeling_llama, \"LlamaFlashAttention2\"):\n    print(\">>> Monkeypatching LlamaFlashAttention2 for DeepSeek-OCR compatibility...\")\n    transformers.models.llama.modeling_llama.LlamaFlashAttention2 = transformers.models.llama.modeling_llama.LlamaAttention","metadata":{"_uuid":"6968d598-691f-46ca-a679-34f42044a150","_cell_guid":"0dd1863a-05b5-4fe9-a037-62a306624dc2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:48:01.343939Z","iopub.execute_input":"2026-01-20T11:48:01.344200Z","iopub.status.idle":"2026-01-20T11:48:29.462324Z","shell.execute_reply.started":"2026-01-20T11:48:01.344165Z","shell.execute_reply":"2026-01-20T11:48:29.461648Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"c7f11344-a483-49e6-844d-37d0ff24a7b0","_cell_guid":"4cef6c25-6f40-48d9-ba63-0c51335dd765","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    PreTrainedModel\n)\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nimport torchvision.transforms as T\nimport warnings\nimport gc\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport json\nimport random\nfrom faker import Faker\nfrom peft import get_peft_model, LoraConfig\nimport types\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"_uuid":"30199b12-a16f-4ddc-b054-e92d516f4e16","_cell_guid":"8577f1c7-749f-4a25-a19f-2e319f114718","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:48:29.463997Z","iopub.execute_input":"2026-01-20T11:48:29.464475Z","iopub.status.idle":"2026-01-20T11:48:32.425332Z","shell.execute_reply.started":"2026-01-20T11:48:29.464449Z","shell.execute_reply":"2026-01-20T11:48:32.424510Z"},"id":"4399efb5","jupyter":{"outputs_hidden":false},"outputId":"c2a9e462-a590-4f01-b30b-c40e53ee20cc"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Projector MLP","metadata":{"_uuid":"48251975-6a26-4c80-80e0-790f748b2fa0","_cell_guid":"43230b91-c7a7-40b9-879f-120bf8822862","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DeepSeekOCRToGOTProjector(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim=1024):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(encoder_dim, decoder_dim * 2),\n            nn.GELU(),\n            nn.Linear(decoder_dim * 2, decoder_dim),\n            nn.LayerNorm(decoder_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"_uuid":"5add0a56-a913-4189-941a-428d8d06b38a","_cell_guid":"a08be259-d9dc-486b-8480-a8fa31117c0f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:48:32.426279Z","iopub.execute_input":"2026-01-20T11:48:32.426875Z","iopub.status.idle":"2026-01-20T11:48:32.431527Z","shell.execute_reply.started":"2026-01-20T11:48:32.426850Z","shell.execute_reply":"2026-01-20T11:48:32.430813Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fusion Model","metadata":{"_uuid":"c24d5176-5d0b-4c76-af9e-6f9be419696b","_cell_guid":"c5a62f97-eb00-4d75-8157-b064b11db129","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DeepSeekOCRToGOTProjector(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim=1024):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(encoder_dim, decoder_dim * 2),\n            nn.GELU(),\n            nn.Linear(decoder_dim * 2, decoder_dim),\n            nn.LayerNorm(decoder_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass DeepSeekGOTFusion(nn.Module):\n    def __init__(self, deepseek_ocr_path, got_path, tokenizer, use_lora=True):\n        super().__init__()\n        warnings.filterwarnings(\"ignore\")\n        frozen_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n        self.vision_dtype = frozen_dtype\n\n        # --- 1. Load DeepSeek Encoder ---\n        print(f\">>> Loading DeepSeek-OCR Encoder...\")\n        tmp_ds = AutoModel.from_pretrained(\n            deepseek_ocr_path,\n            trust_remote_code=True,\n            # device_map=\"cpu\",\n            torch_dtype=frozen_dtype,\n            # low_cpu_mem_usage=True\n        )\n        if hasattr(tmp_ds, \"model\"):\n            base_model = tmp_ds.model\n        else:\n            base_model = tmp_ds\n\n        if hasattr(base_model, \"deep_encoder\"):\n            self.vision_tower = base_model.deep_encoder\n        elif hasattr(base_model, \"vision_model\"):\n            self.vision_tower = base_model.vision_model\n        else:\n            self.vision_tower = base_model\n\n        self.vision_dim = 1024\n        if hasattr(self.vision_tower, \"config\"):\n            self.vision_dim = getattr(self.vision_tower.config, \"hidden_size\", 1024)\n        if self.vision_dim == 1280:\n            self.vision_dim = 1024\n        \n        # self.vision_tower = self.vision_tower.to(\"cuda\")\n        del tmp_ds\n        gc.collect()\n\n        # --- 2. Load GOT-OCR Decoder ---\n        print(f\">>> Loading GOT-OCR Decoder (AutoModel)...\")\n        tmp_got = AutoModel.from_pretrained(\n            got_path,\n            trust_remote_code=True,\n            # device_map=\"cpu\",\n            torch_dtype=frozen_dtype,\n            # low_cpu_mem_usage=True\n        )\n\n        if hasattr(tmp_got, \"language_model\"):\n            self.decoder = tmp_got.language_model\n        else:\n            self.decoder = tmp_got.model\n\n        self.decoder_dim = self.decoder.config.hidden_size\n        # self.decoder = self.decoder.to(\"cuda\")\n\n        # --- 3. Extract LM Head ---\n        self.lm_head = None\n        if hasattr(tmp_got, \"lm_head\"):\n            self.lm_head = tmp_got.lm_head\n        elif hasattr(self.decoder, \"lm_head\"):\n            self.lm_head = self.decoder.lm_head\n        \n        if self.lm_head is None:\n            vocab_size = self.decoder.config.vocab_size\n            self.lm_head = nn.Linear(self.decoder_dim, vocab_size, bias=False)\n            if hasattr(self.decoder, \"embed_tokens\"):\n                self.lm_head.weight = self.decoder.embed_tokens.weight\n            elif hasattr(self.decoder, \"wte\"):\n                self.lm_head.weight = self.decoder.wte.weight\n        \n        self.lm_head = self.lm_head.to(dtype=frozen_dtype) #, device=\"cuda\")\n        del tmp_got\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        # --- 4. ROBUST MONKEY PATCH ---\n        original_forward = self.decoder.forward\n        \n        def patched_forward(input_ids=None, past_key_values=None, attention_mask=None, \n                            token_type_ids=None, position_ids=None, head_mask=None, \n                            inputs_embeds=None, encoder_hidden_states=None, \n                            encoder_attention_mask=None, use_cache=None, \n                            output_attentions=None, output_hidden_states=None, \n                            return_dict=None, labels=None, **kwargs):\n            \n            # STRICT FILTERING: Only pass arguments that GOTQwenModel definitely accepts.\n            # We explicitly DROP 'token_type_ids', 'labels', 'head_mask', etc.\n            \n            return original_forward(\n                input_ids=input_ids,\n                past_key_values=past_key_values,\n                attention_mask=attention_mask,\n                # token_type_ids=token_type_ids,  <-- REMOVED (The cause of your error)\n                position_ids=position_ids,\n                # head_mask=head_mask,            <-- REMOVED\n                inputs_embeds=inputs_embeds,\n                # encoder_hidden_states=...       <-- REMOVED\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict\n            )\n        \n        self.decoder.forward = patched_forward\n\n        # Patch prepare_inputs to also exclude token_type_ids\n        if not hasattr(self.decoder, \"prepare_inputs_for_generation\"):\n            def _prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, **kwargs):\n                return {\n                    \"input_ids\": input_ids,\n                    \"past_key_values\": past_key_values,\n                    \"attention_mask\": attention_mask,\n                    \"inputs_embeds\": kwargs.get(\"inputs_embeds\", None)\n                    # No token_type_ids here either\n                }\n            self.decoder.prepare_inputs_for_generation = types.MethodType(_prepare_inputs_for_generation, self.decoder)\n\n        # --- 5. LoRA Injection ---\n        self.decoder.requires_grad_(False) \n        if use_lora:\n            print(\">>> Injecting LoRA Adapters...\")\n            lora_config = LoraConfig(\n                r=16, \n                lora_alpha=32, \n                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n                lora_dropout=0.05, \n                bias=\"none\", \n                task_type=\"CAUSAL_LM\"\n            )\n            self.decoder = get_peft_model(self.decoder, lora_config)\n            self.decoder.print_trainable_parameters() \n\n        # --- 6. Projector ---\n        self.projector = DeepSeekOCRToGOTProjector(self.vision_dim, self.decoder_dim)\n        self.projector.apply(self._init_weights)\n        \n        self.img_start_id = tokenizer.convert_tokens_to_ids(\"<img>\") \n        self.img_end_id = tokenizer.convert_tokens_to_ids(\"</img>\") \n\n        self.vision_tower.requires_grad_(False)\n        self.lm_head.requires_grad_(False)\n        self.projector.requires_grad_(True) \n        # self.projector = self.projector.to(device=\"cuda\", dtype=torch.float32)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.01)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.ones_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        if hasattr(self.decoder, \"gradient_checkpointing_enable\"):\n            self.decoder.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n\n    def save_pretrained(self, save_directory):\n        if not os.path.exists(save_directory): os.makedirs(save_directory)\n        torch.save(self.projector.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n        if hasattr(self.decoder, \"save_pretrained\"):\n             self.decoder.save_pretrained(save_directory)\n        with open(os.path.join(save_directory, \"config.json\"), \"w\") as f:\n            json.dump({\n                \"vision_dim\": self.vision_dim, \n                \"decoder_dim\": self.decoder_dim,\n                \"architecture\": \"DeepSeekOCRToGOTProjector\"\n            }, f, indent=4)\n\n    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None, **kwargs):\n        # 1. Vision Encode\n        with torch.no_grad():\n            try:\n                vision_out = self.vision_tower(pixel_values.to(self.vision_dtype), patch_embeds=None)\n            except TypeError:\n                vision_out = self.vision_tower(pixel_values.to(self.vision_dtype))\n            \n            if isinstance(vision_out, (tuple, list)):\n                features = vision_out[0]\n            elif hasattr(vision_out, \"last_hidden_state\"):\n                features = vision_out.last_hidden_state\n            else:\n                features = vision_out\n            features = features.detach()\n\n        # 2. Project\n        vision_embeds = self.projector(features.to(torch.float32)).to(self.vision_dtype)\n        B, N, _ = vision_embeds.shape\n        device = vision_embeds.device\n\n        # 3. Text Embeds\n        # Get embeddings from the underlying model (bypass LoRA if needed to find the embedding layer)\n        base_decoder = self.decoder.get_base_model() if hasattr(self.decoder, \"get_base_model\") else self.decoder\n        if hasattr(base_decoder, \"get_input_embeddings\"):\n            input_emb_fn = base_decoder.get_input_embeddings()\n        elif hasattr(base_decoder, \"model\") and hasattr(base_decoder.model, \"embed_tokens\"):\n             input_emb_fn = base_decoder.model.embed_tokens\n        else:\n             input_emb_fn = base_decoder.embed_tokens\n\n        start_embeds = input_emb_fn(torch.tensor([self.img_start_id], device=device)).expand(B, 1, -1)\n        end_embeds = input_emb_fn(torch.tensor([self.img_end_id], device=device)).expand(B, 1, -1)\n        text_embeds = input_emb_fn(input_ids)\n\n        inputs_embeds = torch.cat([start_embeds, vision_embeds, end_embeds, text_embeds], dim=1)\n        \n        vision_len = N + 2\n        full_mask = None\n        if attention_mask is not None:\n            v_mask = torch.ones((B, vision_len), device=device, dtype=attention_mask.dtype)\n            full_mask = torch.cat([v_mask, attention_mask], dim=1)\n\n        # 4. Decoder Call\n        # We pass 'labels' to self.decoder. The monkey patch above will capture it (satisfying LoRA/Trainer)\n        # but NOT pass it to the actual GOTQwenModel (preventing the crash).\n        outputs = self.decoder(\n            input_ids=torch.zeros((B, inputs_embeds.shape[1]), device=device, dtype=torch.long),\n            inputs_embeds=inputs_embeds,\n            attention_mask=full_mask,\n            labels=labels, \n            return_dict=True,\n            use_cache=False \n        )\n        \n        hidden_states = outputs.last_hidden_state\n        relevant_hidden = hidden_states[:, vision_len-1:-1, :].contiguous()\n        \n        if self.lm_head is not None:\n            logits = self.lm_head(relevant_hidden)\n        else:\n            raise ValueError(\"LM Head is not defined.\")\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            labels = labels.to(logits.device)\n            loss = loss_fct(logits.view(-1, self.decoder.config.vocab_size).to(torch.float32), labels.view(-1))\n\n        return CausalLMOutputWithPast(loss=loss, logits=logits if not self.training else None)","metadata":{"_uuid":"d90cabf1-49e1-4472-9ade-e82c5e93af73","_cell_guid":"2b2bafd6-10c4-4f19-a909-b9686a55c3bc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:48:32.432466Z","iopub.execute_input":"2026-01-20T11:48:32.432759Z","iopub.status.idle":"2026-01-20T11:48:32.550290Z","shell.execute_reply.started":"2026-01-20T11:48:32.432737Z","shell.execute_reply":"2026-01-20T11:48:32.549500Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{"_uuid":"a435e61f-a325-446b-9408-03097e18ae6f","_cell_guid":"e677a2b6-ff96-400d-a68e-54c4349ed7b6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class RealTextOCRDataset(Dataset):\n    def __init__(self, tokenizer, num_samples=20000):\n        self.tokenizer = tokenizer\n        self.num_samples = num_samples\n        self.fake = Faker('en_US')  # English generator\n        \n        self.transform = T.Compose([\n            T.Resize((1024, 1024), interpolation=T.InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n                        std=[0.26862954, 0.26130258, 0.27577711])\n        ])\n\n    def __len__(self):\n        return self.num_samples\n\n    def generate_content(self):\n        # Generate varied content types to make the model robust\n        r = random.random()\n        if r < 0.4:\n            # Type 1: Standard Sentences (The easiest for LLMs)\n            return self.fake.sentence(nb_words=10)\n        elif r < 0.7:\n            # Type 2: Addresses (Structured data)\n            return self.fake.address().replace('\\n', ', ')\n        else:\n            # Type 3: Names and Phone numbers\n            return f\"{self.fake.name()} - {self.fake.phone_number()}\"\n\n    def generate_image(self, text):\n        # 1. Random Background (White-ish)\n        bg_color = random.randint(230, 255)\n        img = Image.new('RGB', (1024, 1024), color=(bg_color, bg_color, bg_color))\n        draw = ImageDraw.Draw(img)\n\n        # 2. Font Management\n        try:\n            # Attempt to use a larger, clearer font size\n            font_size = random.randint(40, 80) \n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\", font_size)\n        except IOError:\n            font = ImageFont.load_default()\n\n        # 3. Draw Text (Centered-ish)\n        x = random.randint(50, 100)\n        y = random.randint(200, 500)\n        \n        # Simple text wrapping logic\n        words = text.split()\n        current_line = \"\"\n        for word in words:\n            if (len(current_line) + len(word)) * (font_size * 0.5) > 800:\n                draw.text((x, y), current_line, fill=(0, 0, 0), font=font)\n                y += font_size + 10\n                current_line = word + \" \"\n            else:\n                current_line += word + \" \"\n        \n        # Draw the last line\n        draw.text((x, y), current_line, fill=(0, 0, 0), font=font)\n\n        return img\n\n    def __getitem__(self, idx):\n        text = self.generate_content()\n        image = self.generate_image(text)\n\n        pixel_values = self.transform(image)\n        prompt = f\"OCR: {text}{self.tokenizer.eos_token}\"\n        \n        # Consistent prefix masking logic from Stage 1 setup\n        prefix_enc = self.tokenizer(\"OCR: \", add_special_tokens=False)\n        prefix_len = len(prefix_enc.input_ids)\n\n        encodings = self.tokenizer(\n            prompt,\n            max_length=512,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        \n        input_ids = encodings.input_ids.squeeze(0)\n        attention_mask = encodings.attention_mask.squeeze(0)\n        labels = input_ids.clone()\n        \n        # Mask prefix and padding\n        starts_with_bos = (input_ids[0] == self.tokenizer.bos_token_id)\n        offset = 1 if starts_with_bos else 0\n        labels[:prefix_len + offset] = -100\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }","metadata":{"_uuid":"5564d521-2d68-4cf2-9987-1b06b00b6b93","_cell_guid":"e9946504-7aa5-4059-8f9c-c3d7d4474b05","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:48:32.551348Z","iopub.execute_input":"2026-01-20T11:48:32.551670Z","iopub.status.idle":"2026-01-20T11:48:32.568559Z","shell.execute_reply.started":"2026-01-20T11:48:32.551646Z","shell.execute_reply":"2026-01-20T11:48:32.567893Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training configuration","metadata":{"_uuid":"0fe0d296-d64f-4b36-b5ca-97586af034c2","_cell_guid":"3cad149c-6024-430b-bdad-a52750b2868c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"DEEPSEEK_OCR_PATH = \"deepseek-ai/DeepSeek-OCR\"\nGOT_PATH = \"stepfun-ai/GOT-OCR2_0\"\nOUTPUT_DIR = \"./deepseek_ocr_got_final\"","metadata":{"_uuid":"b253bf8a-41c7-4b54-a7a4-d77a9f468e97","_cell_guid":"8c9d3028-fbac-40d7-beb3-650144c4c1b1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:48:32.569466Z","iopub.execute_input":"2026-01-20T11:48:32.569743Z","iopub.status.idle":"2026-01-20T11:48:32.581146Z","shell.execute_reply.started":"2026-01-20T11:48:32.569711Z","shell.execute_reply":"2026-01-20T11:48:32.580542Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\">>> 1. Loading Tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(GOT_PATH, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(\">>> 2. Initializing RealTextOCRDataset...\")\ntrain_dataset = RealTextOCRDataset(tokenizer, num_samples=20000)\n\ndef collate_fn(batch):\n    return {\n        \"pixel_values\": torch.stack([x['pixel_values'] for x in batch]),\n        \"input_ids\": torch.stack([x['input_ids'] for x in batch]),\n        \"labels\": torch.stack([x['labels'] for x in batch]),\n        \"attention_mask\": torch.stack([x['attention_mask'] for x in batch])\n    }\n    \nprint(\">>> 3. Initializing Fusion Model...\")\nmodel = DeepSeekGOTFusion(\n    DEEPSEEK_OCR_PATH, \n    GOT_PATH, \n    tokenizer, \n)","metadata":{"_uuid":"86458d4e-03e0-48a7-ad3c-6d2d1cf956b7","_cell_guid":"1adc86df-e77d-4329-9da6-77b467880054","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:48:32.582103Z","iopub.execute_input":"2026-01-20T11:48:32.582418Z","iopub.status.idle":"2026-01-20T11:49:30.304695Z","shell.execute_reply.started":"2026-01-20T11:48:32.582396Z","shell.execute_reply":"2026-01-20T11:49:30.303735Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_STEPS = 63\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    learning_rate=1e-5,\n    max_steps=MAX_STEPS,   # Use max_steps for streaming\n    fp16=True,\n    gradient_checkpointing=True,\n    logging_steps=1,\n    save_strategy=\"steps\",\n    save_steps=50,      \n    remove_unused_columns=False,\n    report_to=\"none\",\n    save_safetensors=False,\n    dataloader_pin_memory=False,\n    prediction_loss_only=True,\n    max_grad_norm=0.5,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=5,\n    ddp_find_unused_parameters=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=collate_fn,\n)","metadata":{"_uuid":"abd51587-904f-4f15-a10f-b1b52b425a28","_cell_guid":"94fb08ae-c4cb-484b-9702-6a56aa5af68a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T11:49:30.308193Z","iopub.execute_input":"2026-01-20T11:49:30.308935Z","iopub.status.idle":"2026-01-20T11:49:31.016279Z","shell.execute_reply.started":"2026-01-20T11:49:30.308897Z","shell.execute_reply":"2026-01-20T11:49:31.015692Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{"_uuid":"e0732442-8911-4650-b1ca-4c9713410f66","_cell_guid":"d7b6ba9e-b11c-4fb1-a3b0-483c6e84143f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\">>> 4. Starting Training...\")\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\">>> Trainable Parameters: {trainable_params:,}\")\n\ntrainer.train()\n\nprint(f\">>> 5. Saving Projector to {OUTPUT_DIR}...\")\nmodel.save_pretrained(OUTPUT_DIR)\nprint(\">>> Done.\")","metadata":{"_uuid":"a037c31b-3e61-45fd-9e8b-89c65d6fa1e6","_cell_guid":"2ab11d7a-08fa-48a9-ac54-7f4c44d0dc98","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-20T11:49:31.017682Z","iopub.execute_input":"2026-01-20T11:49:31.018450Z","execution_failed":"2026-01-20T11:51:18.474Z"}},"outputs":[],"execution_count":null}]}