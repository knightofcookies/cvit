{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be6b78f2",
   "metadata": {
    "id": "be6b78f2"
   },
   "outputs": [],
   "source": [
    "# import os, re\n",
    "# if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "#     !pip install unsloth\n",
    "# else:\n",
    "#     # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "#     import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "#     xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "#     !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "#     !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "#     !pip install --no-deps unsloth\n",
    "# !pip install transformers==4.56.2\n",
    "# !pip install --no-deps trl==0.22.2\n",
    "# !pip install jiwer\n",
    "# !pip install einops addict easydict\n",
    "# !pip install verovio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7036ae",
   "metadata": {
    "id": "9d7036ae"
   },
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download, login\n",
    "# login(token='thanks_secret_scanning')\n",
    "# snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4399efb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4399efb5",
    "outputId": "c2a9e462-a590-4f01-b30b-c40e53ee20cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 12:08:58.919285: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1768738138.941274    2674 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1768738138.948009    2674 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1768738138.965181    2674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768738138.965200    2674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768738138.965203    2674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1768738138.965205    2674 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Monkeypatching LlamaFlashAttention2 for DeepSeek-OCR compatibility...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "import torchvision.transforms as T\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "import gc\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Must be set before torch is imported to take effect\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 0. HOTFIX FOR DEEPSEEK-OCR (Transformers >= 4.46 compatibility)\n",
    "#    DeepSeek-OCR's remote code tries to import 'LlamaFlashAttention2' which\n",
    "#    was removed in recent transformers versions. We monkeypatch it.\n",
    "# ==============================================================================\n",
    "import transformers.models.llama.modeling_llama\n",
    "\n",
    "# Check if the class is missing and inject a dummy alias if needed\n",
    "if not hasattr(transformers.models.llama.modeling_llama, \"LlamaFlashAttention2\"):\n",
    "    print(\">>> Monkeypatching LlamaFlashAttention2 for DeepSeek-OCR compatibility...\")\n",
    "    # Map it to the standard LlamaAttention class as a fallback\n",
    "    transformers.models.llama.modeling_llama.LlamaFlashAttention2 = transformers.models.llama.modeling_llama.LlamaAttention\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. THE PROJECTOR (The \"Bridge\")\n",
    "#    Connects DeepSeek's Compressed Vision Tokens to GOT's Decoder\n",
    "# ==============================================================================\n",
    "class DeepSeekOCRToGOTProjector(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim=1024):\n",
    "        super().__init__()\n",
    "        # DeepSeek-OCR tokens are highly compressed (dense information).\n",
    "        # We use a bottleneck MLP to map them to Qwen's embedding space.\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(encoder_dim, decoder_dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(decoder_dim * 2, decoder_dim),\n",
    "            nn.LayerNorm(decoder_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. THE FUSION MODEL (FIXED FOR AMP/FP16 TRAINING)\n",
    "# ==============================================================================\n",
    "class DeepSeekGOTFusion(nn.Module):\n",
    "    def __init__(self, deepseek_ocr_path, got_path, tokenizer):\n",
    "        super().__init__()\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "        # Frozen backbone dtype (Save VRAM)\n",
    "        frozen_dtype = torch.float16\n",
    "        self.vision_dtype = frozen_dtype\n",
    "\n",
    "        # --- A. LOAD DEEPSEEK-OCR ENCODER ---\n",
    "        print(f\">>> Loading DeepSeek-OCR Encoder from: {deepseek_ocr_path}...\")\n",
    "        try:\n",
    "            # Load to CPU first to avoid OOM when both models are loaded\n",
    "            tmp_ds = AutoModel.from_pretrained(\n",
    "                deepseek_ocr_path,\n",
    "                trust_remote_code=True,\n",
    "                device_map=\"cpu\",\n",
    "                torch_dtype=frozen_dtype,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "\n",
    "            if hasattr(tmp_ds, \"model\"):\n",
    "                base_model = tmp_ds.model\n",
    "            else:\n",
    "                base_model = tmp_ds\n",
    "\n",
    "            if hasattr(base_model, \"deep_encoder\"):\n",
    "                self.vision_tower = base_model.deep_encoder\n",
    "            elif hasattr(base_model, \"vision_model\"):\n",
    "                self.vision_tower = base_model.vision_model\n",
    "            else:\n",
    "                raise ValueError(\"Could not find 'deep_encoder'.\")\n",
    "\n",
    "            # Determine Dimension\n",
    "            self.vision_dim = 1024\n",
    "            if hasattr(self.vision_tower, \"config\"):\n",
    "                self.vision_dim = getattr(self.vision_tower.config, \"hidden_size\", 1024)\n",
    "\n",
    "            print(f\">>> Vision Dimension: {self.vision_dim}\")\n",
    "            \n",
    "            # Move only the needed component to GPU\n",
    "            self.vision_tower = self.vision_tower.to(\"cuda\")\n",
    "            \n",
    "            del tmp_ds\n",
    "            if 'base_model' in locals(): del base_model\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR Loading DeepSeek-OCR: {e}\")\n",
    "            raise e\n",
    "\n",
    "        # --- B. LOAD GOT-OCR DECODER ---\n",
    "        print(f\">>> Loading GOT-OCR Decoder from: {got_path}...\")\n",
    "        # Load to CPU first\n",
    "        tmp_got = AutoModel.from_pretrained(\n",
    "            got_path,\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"cpu\",\n",
    "            torch_dtype=frozen_dtype,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "\n",
    "        if hasattr(tmp_got, \"language_model\"):\n",
    "            self.decoder = tmp_got.language_model\n",
    "        else:\n",
    "            self.decoder = tmp_got.model\n",
    "\n",
    "        self.decoder_dim = self.decoder.config.hidden_size\n",
    "        \n",
    "        # Move decoder to GPU\n",
    "        self.decoder = self.decoder.to(\"cuda\")\n",
    "\n",
    "        # --- C. RECREATE LM HEAD ---\n",
    "        self.lm_head = None\n",
    "        if hasattr(tmp_got, \"lm_head\"):\n",
    "            self.lm_head = tmp_got.lm_head\n",
    "        elif hasattr(self.decoder, \"lm_head\"):\n",
    "            self.lm_head = self.decoder.lm_head\n",
    "\n",
    "        if self.lm_head is None:\n",
    "            vocab_size = self.decoder.config.vocab_size\n",
    "            self.lm_head = nn.Linear(self.decoder_dim, vocab_size, bias=False)\n",
    "            if hasattr(self.decoder, \"embed_tokens\"):\n",
    "                self.lm_head.weight = self.decoder.embed_tokens.weight\n",
    "            elif hasattr(self.decoder, \"wte\"):\n",
    "                self.lm_head.weight = self.decoder.wte.weight\n",
    "            \n",
    "        # Move LM Head to GPU\n",
    "        self.lm_head = self.lm_head.to(dtype=frozen_dtype, device=\"cuda\")\n",
    "\n",
    "        del tmp_got\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # --- D. INIT PROJECTOR ---\n",
    "        print(f\">>> Initializing Projector: {self.vision_dim} -> {self.decoder_dim}\")\n",
    "        self.projector = DeepSeekOCRToGOTProjector(self.vision_dim, self.decoder_dim)\n",
    "\n",
    "        # --- E. CONFIG ---\n",
    "        self.img_start_id = tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "        self.img_end_id = tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "\n",
    "        # Freezing\n",
    "        self.vision_tower.requires_grad_(False)\n",
    "        self.decoder.requires_grad_(False)\n",
    "        self.lm_head.requires_grad_(False)\n",
    "        self.projector.requires_grad_(True)\n",
    "\n",
    "        # --- FIX: TRAINABLE PARAMS MUST BE FLOAT32 FOR AMP ---\n",
    "        # The backbones stay fp16 (frozen), but the projector is fp32.\n",
    "        # The Trainer will autocast ops to fp16, but weights stay stable in fp32.\n",
    "        self.projector = self.projector.to(device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "        # Initialize Projector Weights\n",
    "        self.projector.apply(self._init_weights)\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\"\n",
    "        Custom save method because this is a hybrid nn.Module, \n",
    "        not a standard transformers PreTrainedModel.\n",
    "        Focuses on saving the trained projector.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(save_directory):\n",
    "            os.makedirs(save_directory)\n",
    "        \n",
    "        # 1. Save Projector Weights (The only trained part)\n",
    "        torch.save(self.projector.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n",
    "        \n",
    "        # 2. Save a minimal config for reloading\n",
    "        import json\n",
    "        config = {\n",
    "            \"vision_dim\": self.vision_dim,\n",
    "            \"decoder_dim\": self.decoder_dim,\n",
    "            \"architecture\": \"DeepSeekOCRToGOTProjector\"\n",
    "        }\n",
    "        with open(os.path.join(save_directory, \"config.json\"), \"w\") as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "            \n",
    "        print(f\">>> Projector weights and config saved to {save_directory}\")\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            # Use smaller standard deviation for the \"bridge\" to prevent initial divergence\n",
    "            nn.init.trunc_normal_(m.weight, std=0.01)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
    "        if hasattr(self.decoder, \"gradient_checkpointing_enable\"):\n",
    "            self.decoder.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
    "            self.decoder.config.use_cache = False\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None):\n",
    "        # 1. Vision Forward (DeepEncoder)\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                pixel_values = pixel_values.to(dtype=self.vision_dtype)\n",
    "                vision_out = self.vision_tower(pixel_values, patch_embeds=None)\n",
    "            except TypeError:\n",
    "                pixel_values = pixel_values.to(dtype=self.vision_dtype)\n",
    "                vision_out = self.vision_tower(pixel_values)\n",
    "            \n",
    "            if isinstance(vision_out, (tuple, list)):\n",
    "                features = vision_out[0]\n",
    "            elif hasattr(vision_out, \"last_hidden_state\"):\n",
    "                features = vision_out.last_hidden_state\n",
    "            else:\n",
    "                features = vision_out\n",
    "\n",
    "            features = features.detach()\n",
    "\n",
    "        # 2. Project Features\n",
    "        # Scale down vision features initially to match text embedding magnitude\n",
    "        # and prevent attention spikes that lead to NaNs\n",
    "        vision_embeds = self.projector(features.to(dtype=torch.float32))\n",
    "        vision_embeds = vision_embeds.to(dtype=self.vision_dtype)\n",
    "\n",
    "        # 3. Prepare Embeddings\n",
    "        B, N, _ = vision_embeds.shape\n",
    "        device = vision_embeds.device\n",
    "\n",
    "        input_emb_fn = self.decoder.get_input_embeddings()\n",
    "        start_embeds = input_emb_fn(torch.tensor([self.img_start_id], device=device)).expand(B, 1, -1)\n",
    "        end_embeds = input_emb_fn(torch.tensor([self.img_end_id], device=device)).expand(B, 1, -1)\n",
    "        text_embeds = input_emb_fn(input_ids)\n",
    "\n",
    "        # Sequence: [Start, Vision(N), End, Text(L)]\n",
    "        inputs_embeds = torch.cat([start_embeds, vision_embeds, end_embeds, text_embeds], dim=1)\n",
    "\n",
    "        # 4. Attention Mask\n",
    "        vision_len = N + 2\n",
    "        full_attention_mask = None\n",
    "        if attention_mask is not None:\n",
    "            vision_mask = torch.ones((B, vision_len), device=device, dtype=attention_mask.dtype)\n",
    "            full_attention_mask = torch.cat([vision_mask, attention_mask], dim=1)\n",
    "\n",
    "        # 5. Decoder Forward\n",
    "        # Create dummy input_ids to satisfy the GOT model's internal checks\n",
    "        # which access input_ids.shape even when inputs_embeds is provided.\n",
    "        # Since images=None, it won't actually try to use these ids for vision processing.\n",
    "        dummy_input_ids = torch.zeros(\n",
    "            (B, inputs_embeds.shape[1]), \n",
    "            device=device, \n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        outputs = self.decoder(\n",
    "            input_ids=dummy_input_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=full_attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # MEMORY OPTIMIZATION: Only compute logits for the text tokens (loss calculation).\n",
    "        # vision_len = N + 2. We need logits from vision_len-1 to end-1.\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        relevant_hidden = hidden_states[:, vision_len-1:-1, :].contiguous()\n",
    "        logits = self.lm_head(relevant_hidden)\n",
    "\n",
    "        # 6. Loss Calculation (ROBUST VERSION)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            relevant_labels = labels.contiguous()\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # Cast to float32 for stable loss calculation\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.decoder.config.vocab_size).to(torch.float32), \n",
    "                relevant_labels.view(-1)\n",
    "            )\n",
    "\n",
    "        # To prevent DataParallel from gathering a massive logits tensor on GPU 0\n",
    "        # (which causes OOM), we only return logits when not training.\n",
    "        return CausalLMOutputWithPast(loss=loss, logits=logits if not self.training else None)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. THE DATASET (Full Implementation)\n",
    "# ==============================================================================\n",
    "class DeepSeekOCRDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer):\n",
    "        self.data = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # DeepSeek-OCR / CLIP-Large specific normalization\n",
    "        # Note: DeepSeek-OCR natively supports 1024x1024\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((1024, 1024), interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                        std=[0.26862954, 0.26130258, 0.27577711])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # 1. Load Image\n",
    "        image = item['image']\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        pixel_values = self.transform(image)\n",
    "\n",
    "        # 2. Prepare Text\n",
    "        text = item['text']\n",
    "        # DeepSeek/GOT style prompt\n",
    "        prompt = f\"OCR: {text}{self.tokenizer.eos_token}\"\n",
    "\n",
    "        # 3. Tokenize\n",
    "        encodings = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encodings.input_ids.squeeze(0)\n",
    "        attention_mask = encodings.attention_mask.squeeze(0)\n",
    "\n",
    "        # 4. Labels (Mask padding)\n",
    "        labels = input_ids.clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02dd4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.00 GB\n",
      "Reserved: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# 0. CLEANUP FOR RERUNS (Critical for Kaggle/Colab)\n",
    "# This prevents the \"Process has 14.72 GiB memory in use\" error by clearing previous model refs.\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "if 'trainer' in locals(): del trainer\n",
    "if 'model' in locals(): del model\n",
    "if 'training_args' in locals(): del training_args\n",
    "if 'train_dataset' in locals(): del train_dataset\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Final check\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d583205f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 1. Loading Tokenizer...\n",
      ">>> 2. Preparing Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 3. Initializing Fusion Model...\n",
      ">>> Loading DeepSeek-OCR Encoder from: deepseek-ai/DeepSeek-OCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Vision Dimension: 1024\n",
      ">>> Loading GOT-OCR Decoder from: stepfun-ai/GOT-OCR2_0...\n",
      ">>> Initializing Projector: 1024 -> 1024\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION\n",
    "DEEPSEEK_OCR_PATH = \"deepseek-ai/DeepSeek-OCR\"\n",
    "GOT_PATH = \"stepfun-ai/GOT-OCR2_0\"\n",
    "OUTPUT_DIR = \"./deepseek_ocr_got_final\"\n",
    "\n",
    "print(\">>> 1. Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(GOT_PATH, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\">>> 2. Preparing Dataset...\")\n",
    "dataset = load_dataset(\"hezarai/parsynth-ocr-200k\", split=\"train[:5000]\")\n",
    "dataset = dataset.rename_column(\"image_path\", \"image\")\n",
    "train_dataset = DeepSeekOCRDataset(dataset, tokenizer)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack([x['pixel_values'] for x in batch]),\n",
    "        \"input_ids\": torch.stack([x['input_ids'] for x in batch]),\n",
    "        \"labels\": torch.stack([x['labels'] for x in batch]),\n",
    "        \"attention_mask\": torch.stack([x['attention_mask'] for x in batch])\n",
    "    }\n",
    "    \n",
    "print(\">>> 3. Initializing Fusion Model...\")\n",
    "model = DeepSeekGOTFusion(DEEPSEEK_OCR_PATH, GOT_PATH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f87af64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 4. Starting Training...\n",
      ">>> Trainable Parameters: 4,199,424\n",
      "    - projector.net.0.weight: 2,097,152 params\n",
      "    - projector.net.0.bias: 2,048 params\n",
      "    - projector.net.2.weight: 2,097,152 params\n",
      "    - projector.net.2.bias: 1,024 params\n",
      "    - projector.net.3.weight: 1,024 params\n",
      "    - projector.net.3.bias: 1,024 params\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 2:46:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>13.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.428500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>13.389300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>12.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>11.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>9.982900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>9.121600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>8.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>7.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>7.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.675900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>6.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>6.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>6.237200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>6.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.925100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>5.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.065900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>5.596700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>5.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>5.555500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.545900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.372400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>5.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.417200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>5.432300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.145500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>5.338800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.057100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>5.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>5.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.908900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>4.855700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>4.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>5.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>4.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>4.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>4.793900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>4.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>4.843100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>4.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>4.811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>4.794200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>4.659500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>4.578000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>4.505600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>4.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>4.606000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>4.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>4.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>4.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>4.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>4.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.461200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>4.425500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>4.425600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 5. Saving Projector to ./deepseek_ocr_got_final...\n",
      ">>> Projector weights and config saved to ./deepseek_ocr_got_final\n",
      ">>> Done.\n"
     ]
    }
   ],
   "source": [
    "# TRAINING CONFIG\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,  # REDUCED to 1 to fit in T4\n",
    "    gradient_accumulation_steps=8,  # INCREASED to maintain effective batch size 8\n",
    "    learning_rate=5e-5,    # Reduced LR for stability\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,  # ENABLED for T4\n",
    "    bf16=False, # DISABLED for T4\n",
    "    gradient_checkpointing=True, \n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\",\n",
    "    save_safetensors=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    prediction_loss_only=True,  # CRITICAL: Do not gather logits (saves HUGE memory)\n",
    "    max_grad_norm=1.0      # CLIPPING to prevent NaNs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(\">>> 4. Starting Training...\")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\">>> Trainable Parameters: {trainable_params:,}\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"    - {name}: {param.numel():,} params\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(f\">>> 5. Saving Projector to {OUTPUT_DIR}...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "print(\">>> Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca0b2aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer.pt\t   rng_state.pth  scheduler.pt\t      training_args.bin\n",
      "pytorch_model.bin  scaler.pt\t  trainer_state.json\n"
     ]
    }
   ],
   "source": [
    "!ls ./deepseek_ocr_got_final/checkpoint-313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d55ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Freeing disk space by removing checkpoints...\n",
      ">>> Current disk usage:\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/loop1       20G  6.3G   14G  33% /kaggle/working\n",
      ">>> Copying weights and config...\n",
      ">>> Zipping export files...\n",
      "  adding: projector_export/ (stored 0%)\n",
      "  adding: projector_export/config.json (deflated 22%)\n",
      "  adding: projector_export/pytorch_model.bin (deflated 7%)\n",
      "\n",
      ">>> DONE! You can now download DeepSeek_GOT_Projector.zip\n"
     ]
    }
   ],
   "source": [
    "# CLEANUP AND EXPORT ONLY NECESSARY FILES\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# 1. Clear the checkpoints IMMEDIATELY to free space\n",
    "# Each checkpoint has optimizer states and massive redundancy.\n",
    "print(\">>> Freeing disk space by removing checkpoints...\")\n",
    "!rm -rf {OUTPUT_DIR}/checkpoint-*\n",
    "!rm -rf ./checkpoint-313.zip\n",
    "\n",
    "# 2. Check disk space again\n",
    "print(\">>> Current disk usage:\")\n",
    "!df -h .\n",
    "\n",
    "# 3. Create a clean export directory for just the weights and config\n",
    "EXPORT_DIR = \"./projector_export\"\n",
    "if not os.path.exists(EXPORT_DIR):\n",
    "    os.makedirs(EXPORT_DIR)\n",
    "\n",
    "# 4. Copy just the weights and config\n",
    "# Note: These were saved in the root of OUTPUT_DIR at the end of training\n",
    "print(\">>> Copying weights and config...\")\n",
    "shutil.copy(os.path.join(OUTPUT_DIR, \"pytorch_model.bin\"), os.path.join(EXPORT_DIR, \"pytorch_model.bin\"))\n",
    "shutil.copy(os.path.join(OUTPUT_DIR, \"config.json\"), os.path.join(EXPORT_DIR, \"config.json\"))\n",
    "\n",
    "# 5. Zip ONLY the export directory (very small, only ~17MB)\n",
    "print(\">>> Zipping export files...\")\n",
    "!zip -r DeepSeek_GOT_Projector.zip {EXPORT_DIR}\n",
    "\n",
    "print(\"\\n>>> DONE! You can now download DeepSeek_GOT_Projector.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
