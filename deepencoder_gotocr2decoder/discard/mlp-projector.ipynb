{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"gpuType":"T4","provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14544702,"sourceType":"datasetVersion","datasetId":9289627}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install packages\n*inspired from Unsloth's Deepseek-OCR fine-tuning notebook*","metadata":{"_uuid":"03e66fe3-013a-438a-86b7-bab70dc92cd8","_cell_guid":"eeb8feb6-ebe3-43f2-9b17-8d7f73721ee4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2\n!pip install jiwer\n!pip install einops addict easydict\n!pip install verovio\n!pip install faker\n!pip install peft","metadata":{"_uuid":"19a34c33-4b3d-48c2-ab13-a22b5967822e","_cell_guid":"20853027-008f-4459-a876-0bb4335d522f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:08:26.341412Z","iopub.execute_input":"2026-01-20T17:08:26.341671Z","iopub.status.idle":"2026-01-20T17:09:03.417645Z","shell.execute_reply.started":"2026-01-20T17:08:26.341649Z","shell.execute_reply":"2026-01-20T17:09:03.416931Z"},"id":"be6b78f2","jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"Collecting transformers==4.56.2\n  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (3.20.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.56.2) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.56.2) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.56.2) (2026.1.4)\nDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.57.1\n    Uninstalling transformers-4.57.1:\n      Successfully uninstalled transformers-4.57.1\nSuccessfully installed transformers-4.56.2\nCollecting trl==0.22.2\n  Downloading trl-0.22.2-py3-none-any.whl.metadata (11 kB)\nDownloading trl-0.22.2-py3-none-any.whl (544 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m544.8/544.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: trl\nSuccessfully installed trl-0.22.2\nCollecting jiwer\n  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\nDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-4.0.0 rapidfuzz-3.14.3\nRequirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\nCollecting addict\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (1.13)\nDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nInstalling collected packages: addict\nSuccessfully installed addict-2.4.0\nCollecting verovio\n  Downloading verovio-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.2 kB)\nDownloading verovio-5.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: verovio\nSuccessfully installed verovio-5.7.0\nCollecting faker\n  Downloading faker-40.1.2-py3-none-any.whl.metadata (16 kB)\nDownloading faker-40.1.2-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faker\nSuccessfully installed faker-40.1.2\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (26.0rc2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.56.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2026.1.4)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### HF Login for Kaggle","metadata":{"_uuid":"f5c8230b-0307-4bf9-9197-f871f2d1409a","_cell_guid":"c4168e28-f6c3-4bd6-a948-5e373a573aae","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ntoken = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=token)","metadata":{"_uuid":"15726006-b50f-4403-bb62-04ace17bf55d","_cell_guid":"dbc09d59-4911-4354-9a52-4e4030ecfb3f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:09:03.418976Z","iopub.execute_input":"2026-01-20T17:09:03.419242Z","iopub.status.idle":"2026-01-20T17:09:04.158526Z","shell.execute_reply.started":"2026-01-20T17:09:03.419213Z","shell.execute_reply":"2026-01-20T17:09:04.158005Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"### Hotfix (Transformers >= 4.46 compatibility)","metadata":{"_uuid":"c2733041-b7c9-4d47-84fc-585bc529bf39","_cell_guid":"4c2bbbdc-a599-4102-974b-3f0047fe4f15","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import transformers.models.llama.modeling_llama\nif not hasattr(transformers.models.llama.modeling_llama, \"LlamaFlashAttention2\"):\n    print(\">>> Monkeypatching LlamaFlashAttention2 for DeepSeek-OCR compatibility...\")\n    transformers.models.llama.modeling_llama.LlamaFlashAttention2 = transformers.models.llama.modeling_llama.LlamaAttention","metadata":{"_uuid":"6968d598-691f-46ca-a679-34f42044a150","_cell_guid":"0dd1863a-05b5-4fe9-a037-62a306624dc2","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:09:04.160144Z","iopub.execute_input":"2026-01-20T17:09:04.160422Z","iopub.status.idle":"2026-01-20T17:09:35.711957Z","shell.execute_reply.started":"2026-01-20T17:09:04.160400Z","shell.execute_reply":"2026-01-20T17:09:35.711298Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"2026-01-20 17:09:21.041393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768928961.275218      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768928961.341089      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768928961.903327      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768928961.903369      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768928961.903372      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768928961.903375      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":">>> Monkeypatching LlamaFlashAttention2 for DeepSeek-OCR compatibility...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Imports","metadata":{"_uuid":"c7f11344-a483-49e6-844d-37d0ff24a7b0","_cell_guid":"4cef6c25-6f40-48d9-ba63-0c51335dd765","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom transformers import (\n    AutoModel,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    Trainer,\n    TrainingArguments,\n    PreTrainedModel\n)\nfrom transformers.modeling_outputs import CausalLMOutputWithPast\nimport torchvision.transforms as T\nimport warnings\nimport gc\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nimport json\nimport random\nfrom faker import Faker\nfrom peft import get_peft_model, LoraConfig\nimport types\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"","metadata":{"_uuid":"30199b12-a16f-4ddc-b054-e92d516f4e16","_cell_guid":"8577f1c7-749f-4a25-a19f-2e319f114718","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:09:35.712823Z","iopub.execute_input":"2026-01-20T17:09:35.713366Z","iopub.status.idle":"2026-01-20T17:09:38.885220Z","shell.execute_reply.started":"2026-01-20T17:09:35.713341Z","shell.execute_reply":"2026-01-20T17:09:38.884596Z"},"id":"4399efb5","jupyter":{"outputs_hidden":false},"outputId":"c2a9e462-a590-4f01-b30b-c40e53ee20cc"},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Projector MLP","metadata":{"_uuid":"48251975-6a26-4c80-80e0-790f748b2fa0","_cell_guid":"43230b91-c7a7-40b9-879f-120bf8822862","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DeepSeekOCRToGOTProjector(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim=1024):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(encoder_dim, decoder_dim * 2),\n            nn.GELU(),\n            nn.Linear(decoder_dim * 2, decoder_dim),\n            nn.LayerNorm(decoder_dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"_uuid":"5add0a56-a913-4189-941a-428d8d06b38a","_cell_guid":"a08be259-d9dc-486b-8480-a8fa31117c0f","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:09:38.886141Z","iopub.execute_input":"2026-01-20T17:09:38.886707Z","iopub.status.idle":"2026-01-20T17:09:38.892005Z","shell.execute_reply.started":"2026-01-20T17:09:38.886681Z","shell.execute_reply":"2026-01-20T17:09:38.891329Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Fusion Model","metadata":{"_uuid":"c24d5176-5d0b-4c76-af9e-6f9be419696b","_cell_guid":"c5a62f97-eb00-4d75-8157-b064b11db129","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class DeepSeekGOTFusion(nn.Module):\n    def __init__(self, deepseek_ocr_path, got_path, tokenizer, use_lora=True):\n        super().__init__()\n        warnings.filterwarnings(\"ignore\")\n        frozen_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n        self.vision_dtype = frozen_dtype\n\n        # --- 1. Load DeepSeek Encoder ---\n        print(f\">>> Loading DeepSeek-OCR Encoder...\")\n        tmp_ds = AutoModel.from_pretrained(\n            deepseek_ocr_path,\n            trust_remote_code=True,\n            # device_map=\"cpu\",\n            torch_dtype=frozen_dtype,\n            # low_cpu_mem_usage=True\n        )\n        if hasattr(tmp_ds, \"model\"):\n            base_model = tmp_ds.model\n        else:\n            base_model = tmp_ds\n\n        if hasattr(base_model, \"deep_encoder\"):\n            self.vision_tower = base_model.deep_encoder\n        elif hasattr(base_model, \"vision_model\"):\n            self.vision_tower = base_model.vision_model\n        else:\n            self.vision_tower = base_model\n\n        self.vision_dim = 1024\n        if hasattr(self.vision_tower, \"config\"):\n            self.vision_dim = getattr(self.vision_tower.config, \"hidden_size\", 1024)\n        if self.vision_dim == 1280:\n            self.vision_dim = 1024\n        \n        # self.vision_tower = self.vision_tower.to(\"cuda\")\n        del tmp_ds\n        gc.collect()\n\n        # --- 2. Load GOT-OCR Decoder ---\n        print(f\">>> Loading GOT-OCR Decoder (AutoModel)...\")\n        tmp_got = AutoModel.from_pretrained(\n            got_path,\n            trust_remote_code=True,\n            # device_map=\"cpu\",\n            torch_dtype=frozen_dtype,\n            # low_cpu_mem_usage=True\n        )\n\n        if hasattr(tmp_got, \"language_model\"):\n            self.decoder = tmp_got.language_model\n        else:\n            self.decoder = tmp_got.model\n\n        self.decoder_dim = self.decoder.config.hidden_size\n        # self.decoder = self.decoder.to(\"cuda\")\n\n        # --- 3. Extract LM Head ---\n        self.lm_head = None\n        if hasattr(tmp_got, \"lm_head\"):\n            self.lm_head = tmp_got.lm_head\n        elif hasattr(self.decoder, \"lm_head\"):\n            self.lm_head = self.decoder.lm_head\n        \n        if self.lm_head is None:\n            vocab_size = self.decoder.config.vocab_size\n            self.lm_head = nn.Linear(self.decoder_dim, vocab_size, bias=False)\n            if hasattr(self.decoder, \"embed_tokens\"):\n                self.lm_head.weight = self.decoder.embed_tokens.weight\n            elif hasattr(self.decoder, \"wte\"):\n                self.lm_head.weight = self.decoder.wte.weight\n        \n        self.lm_head = self.lm_head.to(dtype=frozen_dtype) #, device=\"cuda\")\n        del tmp_got\n        gc.collect()\n        torch.cuda.empty_cache()\n\n        # --- 4. ROBUST MONKEY PATCH ---\n        original_forward = self.decoder.forward\n        \n        def patched_forward(input_ids=None, past_key_values=None, attention_mask=None, \n                            token_type_ids=None, position_ids=None, head_mask=None, \n                            inputs_embeds=None, encoder_hidden_states=None, \n                            encoder_attention_mask=None, use_cache=None, \n                            output_attentions=None, output_hidden_states=None, \n                            return_dict=None, labels=None, **kwargs):\n            \n            # STRICT FILTERING: Only pass arguments that GOTQwenModel definitely accepts.\n            # We explicitly DROP 'token_type_ids', 'labels', 'head_mask', etc.\n            \n            return original_forward(\n                input_ids=input_ids,\n                past_key_values=past_key_values,\n                attention_mask=attention_mask,\n                # token_type_ids=token_type_ids,  <-- REMOVED (The cause of your error)\n                position_ids=position_ids,\n                # head_mask=head_mask,            <-- REMOVED\n                inputs_embeds=inputs_embeds,\n                # encoder_hidden_states=...       <-- REMOVED\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict\n            )\n        \n        self.decoder.forward = patched_forward\n\n        # Patch prepare_inputs to also exclude token_type_ids\n        if not hasattr(self.decoder, \"prepare_inputs_for_generation\"):\n            def _prepare_inputs_for_generation(self, input_ids, past_key_values=None, attention_mask=None, **kwargs):\n                return {\n                    \"input_ids\": input_ids,\n                    \"past_key_values\": past_key_values,\n                    \"attention_mask\": attention_mask,\n                    \"inputs_embeds\": kwargs.get(\"inputs_embeds\", None)\n                    # No token_type_ids here either\n                }\n            self.decoder.prepare_inputs_for_generation = types.MethodType(_prepare_inputs_for_generation, self.decoder)\n\n        # --- 5. LoRA Injection ---\n        self.decoder.requires_grad_(False) \n        if use_lora:\n            print(\">>> Injecting LoRA Adapters...\")\n            lora_config = LoraConfig(\n                r=16, \n                lora_alpha=32, \n                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], \n                lora_dropout=0.05, \n                bias=\"none\", \n                task_type=\"CAUSAL_LM\"\n            )\n            self.decoder = get_peft_model(self.decoder, lora_config)\n            self.decoder.print_trainable_parameters() \n\n        # --- 6. Projector ---\n        self.projector = DeepSeekOCRToGOTProjector(self.vision_dim, self.decoder_dim)\n        self.projector.apply(self._init_weights)\n        \n        self.img_start_id = tokenizer.convert_tokens_to_ids(\"<img>\") \n        self.img_end_id = tokenizer.convert_tokens_to_ids(\"</img>\") \n\n        self.vision_tower.requires_grad_(False)\n        self.lm_head.requires_grad_(False)\n        self.projector.requires_grad_(True) \n        # self.projector = self.projector.to(device=\"cuda\", dtype=torch.float32)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.01)\n            if m.bias is not None:\n                nn.init.zeros_(m.bias)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.ones_(m.weight)\n            nn.init.zeros_(m.bias)\n\n    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n        if hasattr(self.decoder, \"gradient_checkpointing_enable\"):\n            self.decoder.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n\n    def save_pretrained(self, save_directory):\n        if not os.path.exists(save_directory): os.makedirs(save_directory)\n        torch.save(self.projector.state_dict(), os.path.join(save_directory, \"pytorch_model.bin\"))\n        if hasattr(self.decoder, \"save_pretrained\"):\n             self.decoder.save_pretrained(save_directory)\n        with open(os.path.join(save_directory, \"config.json\"), \"w\") as f:\n            json.dump({\n                \"vision_dim\": self.vision_dim, \n                \"decoder_dim\": self.decoder_dim,\n                \"architecture\": \"DeepSeekOCRToGOTProjector\"\n            }, f, indent=4)\n\n    def forward(self, pixel_values, input_ids, attention_mask=None, labels=None, **kwargs):\n        # 1. Vision Encode\n        with torch.no_grad():\n            try:\n                vision_out = self.vision_tower(pixel_values.to(self.vision_dtype), patch_embeds=None)\n            except TypeError:\n                vision_out = self.vision_tower(pixel_values.to(self.vision_dtype))\n            \n            if isinstance(vision_out, (tuple, list)):\n                features = vision_out[0]\n            elif hasattr(vision_out, \"last_hidden_state\"):\n                features = vision_out.last_hidden_state\n            else:\n                features = vision_out\n            features = features.detach()\n\n        # 2. Project\n        vision_embeds = self.projector(features.to(torch.float32)).to(self.vision_dtype)\n        B, N, _ = vision_embeds.shape\n        device = vision_embeds.device\n\n        # 3. Text Embeds\n        # Get embeddings from the underlying model (bypass LoRA if needed to find the embedding layer)\n        base_decoder = self.decoder.get_base_model() if hasattr(self.decoder, \"get_base_model\") else self.decoder\n        if hasattr(base_decoder, \"get_input_embeddings\"):\n            input_emb_fn = base_decoder.get_input_embeddings()\n        elif hasattr(base_decoder, \"model\") and hasattr(base_decoder.model, \"embed_tokens\"):\n             input_emb_fn = base_decoder.model.embed_tokens\n        else:\n             input_emb_fn = base_decoder.embed_tokens\n\n        start_embeds = input_emb_fn(torch.tensor([self.img_start_id], device=device)).expand(B, 1, -1)\n        end_embeds = input_emb_fn(torch.tensor([self.img_end_id], device=device)).expand(B, 1, -1)\n        text_embeds = input_emb_fn(input_ids)\n\n        inputs_embeds = torch.cat([start_embeds, vision_embeds, end_embeds, text_embeds], dim=1)\n        \n        vision_len = N + 2\n        full_mask = None\n        if attention_mask is not None:\n            v_mask = torch.ones((B, vision_len), device=device, dtype=attention_mask.dtype)\n            full_mask = torch.cat([v_mask, attention_mask], dim=1)\n\n        # 4. Decoder Call\n        # We pass 'labels' to self.decoder. The monkey patch above will capture it (satisfying LoRA/Trainer)\n        # but NOT pass it to the actual GOTQwenModel (preventing the crash).\n        outputs = self.decoder(\n            input_ids=torch.zeros((B, inputs_embeds.shape[1]), device=device, dtype=torch.long),\n            inputs_embeds=inputs_embeds,\n            attention_mask=full_mask,\n            labels=labels, \n            return_dict=True,\n            use_cache=False \n        )\n        \n        hidden_states = outputs.last_hidden_state\n        relevant_hidden = hidden_states[:, vision_len-1:-1, :].contiguous()\n        \n        if self.lm_head is not None:\n            logits = self.lm_head(relevant_hidden)\n        else:\n            raise ValueError(\"LM Head is not defined.\")\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            labels = labels.to(logits.device)\n            loss = loss_fct(logits.view(-1, self.decoder.config.vocab_size).to(torch.float32), labels.view(-1))\n\n        return CausalLMOutputWithPast(loss=loss, logits=logits if not self.training else None)","metadata":{"_uuid":"d90cabf1-49e1-4472-9ade-e82c5e93af73","_cell_guid":"2b2bafd6-10c4-4f19-a909-b9686a55c3bc","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:09:38.893421Z","iopub.execute_input":"2026-01-20T17:09:38.894542Z","iopub.status.idle":"2026-01-20T17:09:38.923290Z","shell.execute_reply.started":"2026-01-20T17:09:38.894504Z","shell.execute_reply":"2026-01-20T17:09:38.922515Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Dataset","metadata":{"_uuid":"a435e61f-a325-446b-9408-03097e18ae6f","_cell_guid":"e677a2b6-ff96-400d-a68e-54c4349ed7b6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class RealTextOCRDataset(Dataset):\n    def __init__(self, tokenizer, num_samples=20000):\n        self.tokenizer = tokenizer\n        self.num_samples = num_samples\n        self.fake = Faker('en_US')  # English generator\n        \n        self.transform = T.Compose([\n            T.Resize((1024, 1024), interpolation=T.InterpolationMode.BICUBIC),\n            T.ToTensor(),\n            T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n                        std=[0.26862954, 0.26130258, 0.27577711])\n        ])\n\n    def __len__(self):\n        return self.num_samples\n\n    def generate_content(self):\n        # Generate varied content types to make the model robust\n        r = random.random()\n        if r < 0.4:\n            # Type 1: Standard Sentences (The easiest for LLMs)\n            return self.fake.sentence(nb_words=10)\n        elif r < 0.7:\n            # Type 2: Addresses (Structured data)\n            return self.fake.address().replace('\\n', ', ')\n        else:\n            # Type 3: Names and Phone numbers\n            return f\"{self.fake.name()} - {self.fake.phone_number()}\"\n\n    def generate_image(self, text):\n        # 1. Random Background (White-ish)\n        bg_color = random.randint(230, 255)\n        img = Image.new('RGB', (1024, 1024), color=(bg_color, bg_color, bg_color))\n        draw = ImageDraw.Draw(img)\n\n        # 2. Font Management\n        try:\n            # Attempt to use a larger, clearer font size\n            font_size = random.randint(40, 80) \n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\", font_size)\n        except IOError:\n            font = ImageFont.load_default()\n\n        # 3. Draw Text (Centered-ish)\n        x = random.randint(50, 100)\n        y = random.randint(200, 500)\n        \n        # Simple text wrapping logic\n        words = text.split()\n        current_line = \"\"\n        for word in words:\n            if (len(current_line) + len(word)) * (font_size * 0.5) > 800:\n                draw.text((x, y), current_line, fill=(0, 0, 0), font=font)\n                y += font_size + 10\n                current_line = word + \" \"\n            else:\n                current_line += word + \" \"\n        \n        # Draw the last line\n        draw.text((x, y), current_line, fill=(0, 0, 0), font=font)\n\n        return img\n\n    def __getitem__(self, idx):\n        text = self.generate_content()\n        image = self.generate_image(text)\n\n        pixel_values = self.transform(image)\n        prompt = f\"OCR: {text}{self.tokenizer.eos_token}\"\n        \n        # Consistent prefix masking logic from Stage 1 setup\n        prefix_enc = self.tokenizer(\"OCR: \", add_special_tokens=False)\n        prefix_len = len(prefix_enc.input_ids)\n\n        encodings = self.tokenizer(\n            prompt,\n            max_length=512,\n            truncation=True,\n            padding=\"max_length\",\n            return_tensors=\"pt\"\n        )\n        \n        input_ids = encodings.input_ids.squeeze(0)\n        attention_mask = encodings.attention_mask.squeeze(0)\n        labels = input_ids.clone()\n        \n        # Mask prefix and padding\n        starts_with_bos = (input_ids[0] == self.tokenizer.bos_token_id)\n        offset = 1 if starts_with_bos else 0\n        labels[:prefix_len + offset] = -100\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"pixel_values\": pixel_values,\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels\n        }","metadata":{"_uuid":"5564d521-2d68-4cf2-9987-1b06b00b6b93","_cell_guid":"e9946504-7aa5-4059-8f9c-c3d7d4474b05","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:09:38.924189Z","iopub.execute_input":"2026-01-20T17:09:38.924459Z","iopub.status.idle":"2026-01-20T17:09:38.942994Z","shell.execute_reply.started":"2026-01-20T17:09:38.924437Z","shell.execute_reply":"2026-01-20T17:09:38.942336Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Training configuration","metadata":{"_uuid":"0fe0d296-d64f-4b36-b5ca-97586af034c2","_cell_guid":"3cad149c-6024-430b-bdad-a52750b2868c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"DEEPSEEK_OCR_PATH = \"deepseek-ai/DeepSeek-OCR\"\nGOT_PATH = \"stepfun-ai/GOT-OCR2_0\"\nOUTPUT_DIR = \"./deepseek_ocr_got_final\"","metadata":{"_uuid":"b253bf8a-41c7-4b54-a7a4-d77a9f468e97","_cell_guid":"8c9d3028-fbac-40d7-beb3-650144c4c1b1","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:09:38.943885Z","iopub.execute_input":"2026-01-20T17:09:38.944148Z","iopub.status.idle":"2026-01-20T17:09:38.961120Z","shell.execute_reply.started":"2026-01-20T17:09:38.944125Z","shell.execute_reply":"2026-01-20T17:09:38.960326Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\">>> 1. Loading Tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(GOT_PATH, trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(\">>> 2. Initializing RealTextOCRDataset...\")\ntrain_dataset = RealTextOCRDataset(tokenizer, num_samples=20000)\n\ndef collate_fn(batch):\n    return {\n        \"pixel_values\": torch.stack([x['pixel_values'] for x in batch]),\n        \"input_ids\": torch.stack([x['input_ids'] for x in batch]),\n        \"labels\": torch.stack([x['labels'] for x in batch]),\n        \"attention_mask\": torch.stack([x['attention_mask'] for x in batch])\n    }\n    \nprint(\">>> 3. Initializing Fusion Model...\")\nmodel = DeepSeekGOTFusion(\n    DEEPSEEK_OCR_PATH, \n    GOT_PATH, \n    tokenizer, \n)","metadata":{"_uuid":"86458d4e-03e0-48a7-ad3c-6d2d1cf956b7","_cell_guid":"1adc86df-e77d-4329-9da6-77b467880054","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:09:38.963122Z","iopub.execute_input":"2026-01-20T17:09:38.963434Z","iopub.status.idle":"2026-01-20T17:10:41.817384Z","shell.execute_reply.started":"2026-01-20T17:09:38.963412Z","shell.execute_reply":"2026-01-20T17:10:41.816613Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":">>> 1. Loading Tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/300 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78512334164b42e0bc55b20b44a1fe90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_qwen.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d96f6382d3784d8a8e37aedeb5f5bac1"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/stepfun-ai/GOT-OCR2_0:\n- tokenization_qwen.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"qwen.tiktoken: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d4f9ba449744efad8a87d63168d69d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/149 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d52d22c0c7ac4bc2b43781641a16ff76"}},"metadata":{}},{"name":"stdout","text":">>> 2. Initializing RealTextOCRDataset...\n>>> 3. Initializing Fusion Model...\n>>> Loading DeepSeek-OCR Encoder...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"698c2163c91e41b5a335a420b1a0f7cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekocr.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c406195087a14e0b8754b14ac61f5cc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"deepencoder.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dc47e3e7a294067ba9bb4dc694a15b2"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- deepencoder.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_deepseekv2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e048f0efe38942c7a7db1144ee6e5263"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_deepseek_v2.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d1723b954b4600a385423a807ca210"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- configuration_deepseek_v2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- modeling_deepseekv2.py\n- configuration_deepseek_v2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"conversation.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e0c4c2f04374b6e9cd1f805b587e037"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-OCR:\n- modeling_deepseekocr.py\n- deepencoder.py\n- modeling_deepseekv2.py\n- conversation.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nYou are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b57a14beadc4861910b8a550e17688a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000001.safetensors:   0%|          | 0.00/6.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3068dabf419f47348136af54e72aa28c"}},"metadata":{}},{"name":"stderr","text":"Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at deepseek-ai/DeepSeek-OCR and are newly initialized: ['model.vision_model.embeddings.position_ids']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":">>> Loading GOT-OCR Decoder (AutoModel)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/986 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73985b1004504d9bb342096268abe902"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_GOT.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97f7aa8d88e44a3aa6e7bbcd5479fbbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"render_tools.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b9e70803b884080931cacdb76f2e77b"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/stepfun-ai/GOT-OCR2_0:\n- render_tools.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"got_vision_b.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c19babef10841b3a39c35538b56ff9c"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/stepfun-ai/GOT-OCR2_0:\n- got_vision_b.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\nA new version of the following files was downloaded from https://huggingface.co/stepfun-ai/GOT-OCR2_0:\n- modeling_GOT.py\n- render_tools.py\n- got_vision_b.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02f2e349f8254199938cc42f3bd12c5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f64c5c2e4e4969a85bacee5e7848a1"}},"metadata":{}},{"name":"stdout","text":">>> Injecting LoRA Adapters...\ntrainable params: 7,569,408 || all params: 568,098,048 || trainable%: 1.3324\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"MAX_STEPS = 63\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    learning_rate=1e-5,\n    max_steps=MAX_STEPS,   # Use max_steps for streaming\n    fp16=True,\n    gradient_checkpointing=True,\n    logging_steps=1,\n    save_strategy=\"steps\",\n    save_steps=50,      \n    remove_unused_columns=False,\n    report_to=\"none\",\n    save_safetensors=False,\n    dataloader_pin_memory=False,\n    prediction_loss_only=True,\n    max_grad_norm=0.5,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=5,\n    ddp_find_unused_parameters=False,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    data_collator=collate_fn,\n)","metadata":{"_uuid":"abd51587-904f-4f15-a10f-b1b52b425a28","_cell_guid":"94fb08ae-c4cb-484b-9702-6a56aa5af68a","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2026-01-20T17:10:41.819128Z","iopub.execute_input":"2026-01-20T17:10:41.819376Z","iopub.status.idle":"2026-01-20T17:10:42.579306Z","shell.execute_reply.started":"2026-01-20T17:10:41.819354Z","shell.execute_reply":"2026-01-20T17:10:42.578601Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# Train","metadata":{"_uuid":"e0732442-8911-4650-b1ca-4c9713410f66","_cell_guid":"d7b6ba9e-b11c-4fb1-a3b0-483c6e84143f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"print(\">>> 4. Starting Training...\")\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\">>> Trainable Parameters: {trainable_params:,}\")\n\ntrainer.train()\n\nprint(f\">>> 5. Saving Projector to {OUTPUT_DIR}...\")\nmodel.save_pretrained(OUTPUT_DIR)\nprint(\">>> Done.\")","metadata":{"_uuid":"a037c31b-3e61-45fd-9e8b-89c65d6fa1e6","_cell_guid":"2ab11d7a-08fa-48a9-ac54-7f4c44d0dc98","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2026-01-20T17:10:42.580192Z","iopub.execute_input":"2026-01-20T17:10:42.580486Z","iopub.status.idle":"2026-01-20T17:32:45.851668Z","shell.execute_reply.started":"2026-01-20T17:10:42.580463Z","shell.execute_reply":"2026-01-20T17:32:45.851071Z"}},"outputs":[{"name":"stdout","text":">>> 4. Starting Training...\n>>> Trainable Parameters: 11,768,832\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 21:32, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>9.592200</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>12.387600</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>9.254800</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>12.354400</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>8.217500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>10.826000</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>8.729400</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>12.686800</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>16.223900</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>8.381000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>17.503100</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>8.403900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>9.020600</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>17.731600</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>7.759400</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>8.687000</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>17.692900</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>19.311500</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>7.275800</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>10.161200</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>20.262300</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>18.384800</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>18.180400</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>16.362900</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>15.566800</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>14.245100</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>14.437000</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>13.323900</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>13.336600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>14.880400</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>13.469700</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>13.086500</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>12.120300</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>16.256600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>8.665900</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>15.673100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>15.881300</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>14.390000</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>8.130900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>10.906300</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>7.576200</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>7.784200</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>8.471300</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>8.375600</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>16.242600</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>16.812900</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>7.116000</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>10.281700</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>8.313000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>11.277900</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>9.162600</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>8.930500</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>10.717300</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>17.141300</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>15.517000</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>9.513900</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>7.934800</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>9.549700</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>8.549200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>7.048400</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>8.271900</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>15.546000</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>16.430300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":">>> 5. Saving Projector to ./deepseek_ocr_got_final...\n>>> Done.\n","output_type":"stream"}],"execution_count":11}]}